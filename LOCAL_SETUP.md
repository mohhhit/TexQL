# üéâ Local Training Setup Complete!

You now have everything needed to train models on your GTX 1060 3GB!

## üìÅ New Files Created for Local Training

### 1. **train_local.py** - Main Training Script
   - Optimized for 3GB GPU memory
   - Uses smaller model (flan-t5-small - 80M params)
   - Batch size: 2 with gradient accumulation
   - Mixed precision (FP16) training
   - Gradient checkpointing enabled
   - Memory-efficient Adafactor optimizer

### 2. **train.bat** - Windows Batch Script
   - Interactive menu for easy training
   - Choose SQL, MongoDB, or both
   - Automatic error handling
   - Windows-friendly interface

### 3. **check_gpu.py** - GPU Setup Validator
   - Checks CUDA availability
   - Verifies GPU memory
   - Tests GPU training capability
   - Recommends optimal settings
   - Checks all dependencies

### 4. **LOCAL_TRAINING.md** - Complete Guide
   - Detailed instructions
   - Troubleshooting section
   - Performance expectations
   - Command reference
   - Tips and tricks

## üöÄ Quick Start (3 Steps)

### **Step 1: Verify Setup**
```powershell
python check_gpu.py
```

Expected output:
```
‚úÖ Python 3.x detected
‚úÖ PyTorch installed
‚úÖ CUDA available
   GPU Name: NVIDIA GeForce GTX 1060 3GB
   GPU Memory: 3.0 GB
‚úÖ All dependencies installed
‚úÖ Training data found
```

### **Step 2: Generate Data** (if not done)
```powershell
python data_generation.py
```

### **Step 3: Start Training**

**Option A - Interactive (Recommended):**
```powershell
train.bat
```
Then select:
- Option 1: Train SQL model
- Option 2: Train MongoDB model  
- Option 3: Train both (sequential)

**Option B - Command Line:**
```powershell
# SQL model
python train_local.py --target sql

# MongoDB model
python train_local.py --target mongodb
```

## ‚è±Ô∏è Time Estimates

- **Per Epoch**: 2-3 hours
- **10 Epochs**: 20-30 hours
- **Both Models**: 40-60 hours total

üí° **Tip**: Train overnight! Start before bed, wake up to a trained model.

## üéØ What's Different from Colab?

| Feature | Local (Your PC) | Colab Version |
|---------|----------------|---------------|
| **Model** | flan-t5-small (80M) | flan-t5-base (248M) |
| **Batch Size** | 2 | 8 |
| **Sequence Length** | 128/256 | 256/512 |
| **Memory Used** | ~2.8 GB | ~6-8 GB |
| **Training Time** | 20-30 hrs | 5-10 hrs |
| **Accuracy** | 70-80% | 75-85% |
| **Setup** | One-time | Per session |
| **Limitations** | Hardware | 12hr session limit |

## üîß Optimizations Applied

Your GTX 1060 has limited VRAM, so I've optimized:

1. **‚úÖ Smaller Model**: flan-t5-small instead of flan-t5-base
2. **‚úÖ Small Batch**: 2 instead of 8
3. **‚úÖ Gradient Accumulation**: Simulates larger batch (effective: 16)
4. **‚úÖ FP16 Training**: Halves memory usage
5. **‚úÖ Gradient Checkpointing**: Trade speed for memory
6. **‚úÖ Shorter Sequences**: 128/256 instead of 256/512
7. **‚úÖ Efficient Optimizer**: Adafactor instead of Adam
8. **‚úÖ Auto-saving**: Checkpoints after each epoch

## üìä Expected Results

### Training Progress:
```
Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Loss: 0.850
Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Loss: 0.612
Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Loss: 0.485
...
Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Loss: 0.285
```

### Final Metrics:
- **Training Loss**: ~0.25-0.35 (lower is better)
- **Validation Loss**: ~0.30-0.45
- **Exact Match Accuracy**: 70-80%

### Example Output:
```
Natural Language: "Show all employees with salary greater than 50000"
Generated SQL: SELECT * FROM employees WHERE salary > 50000;
‚úì Correct!
```

## üõ°Ô∏è Troubleshooting Quick Reference

### ‚ùå "CUDA out of memory"
```powershell
python train_local.py --target sql --batch-size 1
```

### ‚ùå "CUDA not available"
```powershell
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### ‚ùå "Training too slow"
- This is normal for GTX 1060
- Train overnight (expected: 20-30 hours)
- Or use Google Colab for faster training

### ‚ùå "Low accuracy after training"
- Train more epochs: `--epochs 15`
- Generate more data (10,000 samples)
- Or use Colab with larger model

## üìà Monitoring Training

### During Training:
```powershell
# In another terminal, monitor GPU
nvidia-smi -l 1
```

You should see:
- **GPU Utilization**: 90-100%
- **Memory Used**: 2.5-2.8 GB / 3.0 GB
- **Temperature**: 60-75¬∞C (normal)

### Training Output:
```
Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [2:15:30<00:00]
Epoch 1/10 - Train Loss: 0.642, Val Loss: 0.584
‚úÖ Model checkpoint saved
```

## üéì Recommendations

### For Your Assignment:

**Option 1 - Local Training (Demonstrates Understanding)**
- Shows you can optimize for limited hardware
- Proves understanding of memory constraints
- Good for portfolio/resume
- Takes longer but runs anytime

**Option 2 - Colab Training (Better Results)**
- Higher accuracy (better grades?)
- Faster training (less waiting)
- Larger model (more impressive)
- Standard approach

**Option 3 - Hybrid (Best of Both)**
1. Use `check_gpu.py` to show local setup
2. Train small model locally to learn
3. Final training on Colab for best results
4. Show understanding of both approaches

## üìÅ Output Structure

After training, you'll have:

```
models/
‚îú‚îÄ‚îÄ texql-sql-YYYYMMDD_HHMMSS/    # Training checkpoint
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-1000/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-2000/
‚îÇ   ‚îî‚îÄ‚îÄ logs/
‚îÇ
‚îî‚îÄ‚îÄ texql-sql-final/               # Final model
    ‚îú‚îÄ‚îÄ pytorch_model.bin          # ~320 MB
    ‚îú‚îÄ‚îÄ config.json
    ‚îú‚îÄ‚îÄ tokenizer.json
    ‚îî‚îÄ‚îÄ special_tokens_map.json
```

## ‚úÖ Checklist

Before starting training:
- [ ] Run `check_gpu.py` ‚Üí All green checkmarks
- [ ] Data generated (5000 samples in `data/`)
- [ ] 10+ GB free disk space
- [ ] No other GPU apps running (Chrome, games, etc.)
- [ ] Stable power supply
- [ ] Plan to leave PC on for 24+ hours

During training:
- [ ] Watch first epoch complete (~2-3 hours)
- [ ] Check GPU usage with `nvidia-smi`
- [ ] Monitor temperatures (should be < 80¬∞C)
- [ ] Loss is decreasing each epoch

After training:
- [ ] Model saved to `models/texql-{type}-final/`
- [ ] Test with inference script
- [ ] Run in Streamlit app
- [ ] Calculate accuracy metrics

## üéØ Next Steps

1. **Run Setup Check**:
   ```powershell
   python check_gpu.py
   ```

2. **Start Training** (choose one):
   ```powershell
   # Interactive
   train.bat
   
   # Or command line
   python train_local.py --target sql
   ```

3. **Monitor Progress**:
   ```powershell
   # In another terminal
   nvidia-smi -l 1
   ```

4. **Test Model** (after training):
   ```powershell
   python inference.py --model-path models/texql-sql-final --type sql --interactive
   ```

5. **Run App**:
   ```powershell
   streamlit run app.py
   ```

## üí° Pro Tips

1. **Train overnight**: Start before sleep, check in morning
2. **One model at a time**: Don't train both simultaneously
3. **Save power settings**: Prevent PC from sleeping
4. **Monitor first epoch**: Ensure no errors before leaving
5. **Keep GPU cool**: Ensure good ventilation

## üÜò Need Help?

If issues arise:
1. Check [LOCAL_TRAINING.md](LOCAL_TRAINING.md) - Detailed guide
2. Run `python check_gpu.py` - Diagnose problems
3. Try batch size 1 - If memory issues
4. Consider Colab - If local too slow

## üéâ You're Ready!

Everything is optimized for your GTX 1060 3GB. 

**Start training now:**
```powershell
python check_gpu.py  # Verify setup
train.bat            # Start training
```

**Or read the full guide:**
- [LOCAL_TRAINING.md](LOCAL_TRAINING.md) - Complete instructions
- [README.md](README.md) - Project overview

---

**Good luck with your training! Your model will be ready in ~24 hours! üöÄ**
